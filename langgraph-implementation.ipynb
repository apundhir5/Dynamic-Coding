{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langgraph graphviz langchain langchain_ollama --q\n",
    "%pip install langchain-openai langchain_community --q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import networkx as nx\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import TypedDict, Optional\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "import json\n",
    "import re\n",
    "from typing_extensions import Annotated\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# Ollama\n",
    "# llm = OllamaLLM(model=os.getenv(\"OLLAMA_MODEL\"), base_url=os.getenv(\"OLLAMA_BASE_URL\"))\n",
    "\n",
    "# NTTH\n",
    "from genai_wrapper import NTTHWrapperChatLLM\n",
    "llm = NTTHWrapperChatLLM(\n",
    "    id=os.getenv(\"NTTH_ID\"),\n",
    "    secret=os.getenv(\"NTTH_SECRET\"),\n",
    "    model_name=os.getenv(\"NTTH_MODEL\"),\n",
    "    provider=os.getenv(\"NTTH_PROVIDER\"),\n",
    "    base_url=os.getenv(\"NTTH_BASE_URL\")\n",
    ")\n",
    "\n",
    "# OpenAI\n",
    "# MODEL = os.getenv(\"MODEL\")\n",
    "# OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "# llm = ChatOpenAI(model=MODEL, openai_api_key=OPENAI_API_KEY, temperature=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 1: Define the State Schema\n",
    "class PrototypeState(TypedDict):\n",
    "    input_requirements: Annotated[str, \"single\"]\n",
    "    frontend_spec: Annotated[Optional[str], \"multi\"]\n",
    "    backend_spec: Annotated[Optional[str], \"multi\"]\n",
    "    frontend_code: Annotated[Optional[str], \"multi\"]\n",
    "    backend_code: Annotated[Optional[str], \"multi\"]\n",
    "    frontend_valid: Annotated[bool, \"multi\"]\n",
    "    backend_valid: Annotated[bool, \"multi\"]\n",
    "    frontend_feedback: Annotated[Optional[str], \"multi\"]\n",
    "    backend_feedback: Annotated[Optional[str], \"multi\"]\n",
    "    frontend_attempts: Annotated[int, \"multi\"]  # Tracks frontend iterations\n",
    "    backend_attempts: Annotated[int, \"multi\"]  # Tracks backend iterations\n",
    "    deployment_ready: Annotated[bool, \"multi\"]\n",
    "\n",
    "def get_llm_response(llm, prompt):\n",
    "    \"\"\"Helper function to handle different LLM response formats\"\"\"\n",
    "    response = llm.invoke(prompt)\n",
    "    if isinstance(llm, ChatOpenAI):\n",
    "        return response.content\n",
    "    elif isinstance(llm, NTTHWrapperChatLLM):\n",
    "        response = llm.invoke([HumanMessage(content=prompt)])\n",
    "        return response.content\n",
    "    return response\n",
    "\n",
    "def extract_section(text: str, section: str) -> str:\n",
    "    \"\"\"Extracts frontend/backend sections from LLM response using regex.\"\"\"\n",
    "    pattern = rf\"{section}.*?:\\s*(.*?)(?=\\n\\n|$)\"  # Matches until next blank line or end\n",
    "    match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)\n",
    "    return match.group(1).strip() if match else f\"Error extracting {section} specifications.\"\n",
    "\n",
    "def frontend_requirement_agent(state: PrototypeState) -> PrototypeState:\n",
    "    \"\"\"Extracts frontend-specific requirements manually (without JSON).\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are a software architect. Extract only the FRONTEND specifications from the given project requirements.\n",
    "\n",
    "    **Ensure your response follows this format:**\n",
    "    Frontend Specifications:\n",
    "    - Item 1\n",
    "    - Item 2\n",
    "    - ...\n",
    "\n",
    "    **Project Requirements:** {state['input_requirements']}\n",
    "    \"\"\"\n",
    "    \n",
    "    response = get_llm_response(llm, prompt)\n",
    "    frontend_spec = extract_section(response, \"Frontend Specifications\")\n",
    "    \n",
    "    return {\"frontend_spec\": frontend_spec}\n",
    "\n",
    "def backend_requirement_agent(state: PrototypeState) -> PrototypeState:\n",
    "    \"\"\"Extracts backend-specific requirements manually (without JSON).\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are a software architect. Extract only the BACKEND specifications from the given project requirements.\n",
    "\n",
    "    **Ensure your response follows this format:**\n",
    "    Backend Specifications:\n",
    "    - Item 1\n",
    "    - Item 2\n",
    "    - ...\n",
    "\n",
    "    **Project Requirements:** {state['input_requirements']}\n",
    "    \"\"\"\n",
    "    \n",
    "    response = get_llm_response(llm, prompt)\n",
    "    backend_spec = extract_section(response, \"Backend Specifications\")\n",
    "    \n",
    "    return {\"backend_spec\": backend_spec}\n",
    "\n",
    "# Step 3: Development & Validation Agents (Same as before)\n",
    "def frontend_agent(state: PrototypeState) -> PrototypeState:\n",
    "    \"\"\"Generates frontend code based on specifications using LLM.\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are a frontend developer. Write a clean, modular frontend component based on the following specifications:\n",
    "    \n",
    "    {state['frontend_spec']}\n",
    "    \n",
    "    Ensure the code follows best practices.\n",
    "    \"\"\"\n",
    "    return {\"frontend_code\": get_llm_response(llm, prompt)}\n",
    "\n",
    "def backend_agent(state: PrototypeState) -> PrototypeState:\n",
    "    \"\"\"Generates backend code based on specifications using LLM.\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are a backend developer. Write a clean, modular backend API or service based on the following specifications:\n",
    "    \n",
    "    {state['backend_spec']}\n",
    "    \n",
    "    Ensure the code is scalable and follows best practices.\n",
    "    \"\"\"\n",
    "    return {\"backend_code\": get_llm_response(llm, prompt)}\n",
    "\n",
    "def frontend_validation_agent(state: PrototypeState) -> PrototypeState:\n",
    "    \"\"\"Validates frontend code using LLM with stricter validation handling.\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are a senior frontend developer. Validate the following frontend code:\n",
    "\n",
    "    {state['frontend_code']}\n",
    "\n",
    "    **Response Format:**\n",
    "    - If the code is correct, return exactly: \"Valid Frontend Code.\"\n",
    "    - If the code is incorrect, return: \"Invalid Frontend Code: [detailed explanation]\".\n",
    "\n",
    "    **Your response should be in one of the two formats only. Do not include extra text.**\n",
    "    \"\"\"\n",
    "    \n",
    "    response = get_llm_response(llm, prompt).strip()\n",
    "\n",
    "    # Ensure we only mark valid if response is exactly \"Valid Frontend Code.\"\n",
    "    frontend_valid = response.strip() == \"Valid Frontend Code.\"\n",
    "\n",
    "    return {\n",
    "        \"frontend_valid\": frontend_valid,\n",
    "        \"frontend_feedback\": response\n",
    "    }\n",
    "\n",
    "def backend_validation_agent(state: PrototypeState) -> PrototypeState:\n",
    "    \"\"\"Validates backend code using LLM with stricter validation handling.\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are a senior backend developer. Validate the following backend code:\n",
    "\n",
    "    {state['backend_code']}\n",
    "\n",
    "    **Response Format:**\n",
    "    - If the code is correct, return exactly: \"Valid Backend Code.\"\n",
    "    - If the code is incorrect, return: \"Invalid Backend Code: [detailed explanation]\".\n",
    "\n",
    "    **Your response should be in one of the two formats only. Do not include extra text.**\n",
    "    \"\"\"\n",
    "    \n",
    "    response = get_llm_response(llm, prompt).strip()\n",
    "\n",
    "    backend_valid = response == \"Valid Backend Code.\"\n",
    "\n",
    "    return {\n",
    "        \"backend_valid\": backend_valid,\n",
    "        \"backend_feedback\": response\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1268557f0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = StateGraph(PrototypeState)\n",
    "\n",
    "# Add nodes (Split requirement agent into two)\n",
    "graph.add_node(\"frontend_requirement_agent\", frontend_requirement_agent)\n",
    "graph.add_node(\"backend_requirement_agent\", backend_requirement_agent)\n",
    "graph.add_node(\"frontend_agent\", frontend_agent)\n",
    "graph.add_node(\"backend_agent\", backend_agent)\n",
    "graph.add_node(\"frontend_validation_agent\", frontend_validation_agent)\n",
    "graph.add_node(\"backend_validation_agent\", backend_validation_agent)\n",
    "\n",
    "# # Define edges (Frontend and Backend run independently)\n",
    "graph.add_edge(START, \"frontend_requirement_agent\")\n",
    "graph.add_edge(START, \"backend_requirement_agent\")\n",
    "graph.add_edge(\"frontend_requirement_agent\", \"frontend_agent\")\n",
    "graph.add_edge(\"backend_requirement_agent\", \"backend_agent\")\n",
    "graph.add_edge(\"frontend_agent\", \"frontend_validation_agent\")\n",
    "graph.add_edge(\"backend_agent\", \"backend_validation_agent\")\n",
    "\n",
    "# Step 5: Prevent Infinite Loops with Limits\n",
    "MAX_ATTEMPTS = 3  # Define max iterations per agent\n",
    "\n",
    "def frontend_iteration(state: PrototypeState):\n",
    "    if state['frontend_valid']:\n",
    "        return END  # Stop if valid\n",
    "    \n",
    "    state['frontend_attempts'] += 1\n",
    "    if state['frontend_attempts'] >= MAX_ATTEMPTS:\n",
    "        print(\"⚠️ Frontend validation failed too many times. Stopping...\")\n",
    "        return END  # Stop looping after max attempts\n",
    "    return \"frontend_agent\"  # Retry frontend generation\n",
    "\n",
    "def backend_iteration(state: PrototypeState):\n",
    "    if state['backend_valid']:\n",
    "        return END  # Stop if valid\n",
    "    \n",
    "    state['backend_attempts'] += 1\n",
    "    if state['backend_attempts'] >= MAX_ATTEMPTS:\n",
    "        print(\"⚠️ Backend validation failed too many times. Stopping...\")\n",
    "        return END  # Stop looping after max attempts\n",
    "    return \"backend_agent\"  # Retry backend generation\n",
    "\n",
    "graph.add_conditional_edges(\"frontend_validation_agent\", frontend_iteration)\n",
    "graph.add_conditional_edges(\"backend_validation_agent\", backend_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "compiled_graph = graph.compile()\n",
    "Image(compiled_graph.get_graph().draw_mermaid_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial state with input requirements\n",
    "initial_state = PrototypeState(\n",
    "    input_requirements=\"A web application that allows users to register and view a dashboard with their profile details.\",\n",
    "    frontend_spec=None,\n",
    "    backend_spec=None,\n",
    "    frontend_code=None,\n",
    "    backend_code=None,\n",
    "    frontend_valid=False,\n",
    "    backend_valid=False,\n",
    "    frontend_feedback=None,\n",
    "    backend_feedback=None,\n",
    "    frontend_attempts=0,  # Initialize attempt counters\n",
    "    backend_attempts=0\n",
    ")\n",
    "\n",
    "# Execute the graph\n",
    "final_state = compiled_graph.invoke(initial_state, {\"recursion_limit\": 50})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'backend_attempts': 0,\n",
      " 'backend_code': None,\n",
      " 'backend_feedback': None,\n",
      " 'backend_spec': None,\n",
      " 'backend_valid': False,\n",
      " 'frontend_attempts': 0,\n",
      " 'frontend_code': None,\n",
      " 'frontend_feedback': None,\n",
      " 'frontend_spec': '- Implement a responsive user registration form.\\n'\n",
      "                  '- Design a user-friendly login page with validation.\\n'\n",
      "                  '- Create a dashboard for users after they log in.\\n'\n",
      "                  '- Display user profile details on the dashboard.\\n'\n",
      "                  '- Ensure all pages are accessible and optimized for various '\n",
      "                  'screen sizes.\\n'\n",
      "                  '- Incorporate interactive elements for better user '\n",
      "                  'engagement.\\n'\n",
      "                  '- Use a consistent color scheme and typography across the '\n",
      "                  'frontend.',\n",
      " 'frontend_valid': False,\n",
      " 'input_requirements': 'A web application that allows users to register and '\n",
      "                       'view a dashboard with their profile details.'}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pprint(final_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
